{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNIfIfK15t1kR1iMwMgon9U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NazmusSaad/Medical-Assistant-LLM/blob/main/DynamicRagWIthFineTunedLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irl4ut_VpdT5"
      },
      "outputs": [],
      "source": [
        "!pip install duckduckgo-search beautifulsoup4 requests langchain langchain-community chromadb sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers -U\n",
        "!pip install accelerate -U\n",
        "!pip install trl\n",
        "!pip install bitsandbytes # for quantization\n",
        "!pip install peft # for LoRA"
      ],
      "metadata": {
        "id": "BLo0gimOrk25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema.document import Document\n",
        "import time"
      ],
      "metadata": {
        "id": "2lu-EsxIsVGX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dynamic RAG Template: Web Search + Scrape + Chunk + ChromaDB\n",
        "\n",
        "# Step 1: Search the web\n",
        "def search_web(query, max_results=3):\n",
        "    ddgs = DDGS()\n",
        "    results = []\n",
        "\n",
        "    # get the titles and links to the websites and store them in results\n",
        "    for r in ddgs.text(query, max_results=max_results):\n",
        "        results.append({\"title\": r[\"title\"], \"href\": r[\"href\"]})\n",
        "    return results\n",
        "\n",
        "# Step 2: Scrape the content from the URLs\n",
        "def scrape_content(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        res = requests.get(url, headers=headers, timeout=5)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "        # Extract paragraphs only\n",
        "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed to scrape {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 3: Chunk the scraped text into LangChain Documents\n",
        "def chunk_text(text, source):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = splitter.create_documents([text], metadatas=[{\"source\": source}])\n",
        "    return chunks\n",
        "\n",
        "# Step 4: Embed and store in Chroma\n",
        "def build_chroma_db(docs, persist_dir=\"chroma_db\"):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    db = Chroma.from_documents(docs, embedding=embeddings, persist_directory=persist_dir)\n",
        "    return db\n",
        "\n",
        "# === FULL PIPELINE ===\n",
        "def dynamic_rag_pipeline(query):\n",
        "    print(f\"🔎 Searching web for: {query}\")\n",
        "    results = search_web(query)\n",
        "\n",
        "    all_docs = []\n",
        "    for r in results:\n",
        "        print(f\"🌐 Scraping: {r['href']}\")\n",
        "        text = scrape_content(r['href'])\n",
        "        if text:\n",
        "            chunks = chunk_text(text, r[\"href\"])\n",
        "            all_docs.extend(chunks)\n",
        "        time.sleep(1.5)  # avoid rate-limiting\n",
        "\n",
        "    if not all_docs:\n",
        "        print(\"❌ No content could be retrieved.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"✅ Retrieved {len(all_docs)} chunks. Storing in vector DB...\")\n",
        "    vectordb = build_chroma_db(all_docs)\n",
        "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "    return retriever\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O-LxP1utrlGh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()  # This will prompt you to paste your HF token interactively"
      ],
      "metadata": {
        "id": "3rNk53h3waSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the fine tuned model\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "HF_REPO = \"Nazmoose/MedLlama-LoRA\"\n",
        "BASE_REPO = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "def load_model():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load tokenizer from your LoRA adapter repo (inherits from base)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(HF_REPO, token=True)  # set `token=True` for Colab auth\n",
        "\n",
        "    # Load base LLaMA model in 4-bit\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_REPO,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        token=True\n",
        "    )\n",
        "\n",
        "    # Load LoRA adapters into base model\n",
        "    model = PeftModel.from_pretrained(base_model, HF_REPO, token=True)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n"
      ],
      "metadata": {
        "id": "oMOuKk_IwBTQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to generate a clean response\n",
        "def clean_response(raw_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts only the first ###Answer: section from the model output.\n",
        "    \"\"\"\n",
        "    if \"###Answer:\" in raw_text:\n",
        "        answer_section = raw_text.split(\"###Answer:\")[1].strip()\n",
        "        # Optionally stop at next metadata marker\n",
        "        for stop_token in [\"###Rationale\", \"###Source\", \"###Tags\", \"###Used in\", \"###Context\"]:\n",
        "            if stop_token in answer_section:\n",
        "                answer_section = answer_section.split(stop_token)[0].strip()\n",
        "                break\n",
        "        return answer_section\n",
        "    else:\n",
        "        return raw_text.strip()\n",
        "\n",
        "def generate_clean_with_rag(user_msg, retriever, tokenizer, model):\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"You are MedLLaMA, a model fine-tuned for clinical Q&A. \"\n",
        "        \"Respond with medically relevant answers but do not provide professional advice. \"\n",
        "        \"Use the provided context to answer accurately.\"\n",
        "    )\n",
        "\n",
        "    # === RAG integration ===\n",
        "    retrieved_docs = retriever.get_relevant_documents(user_msg)\n",
        "    rag_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST] <<SYS>>\n",
        "{SYSTEM_PROMPT}\n",
        "<</SYS>>\n",
        "\n",
        "Context:\n",
        "{rag_context}\n",
        "\n",
        "Question:\n",
        "{user_msg}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    print(clean_response(response.strip()))\n"
      ],
      "metadata": {
        "id": "W_s9vvN3vFYg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_pipeline():\n",
        "  # Get user query\n",
        "  user_msg = input(\"Ask a medical question:\\n> \")\n",
        "\n",
        "  # Search the web for relevant documents and return the retriever\n",
        "  retriever = dynamic_rag_pipeline(user_msg)\n",
        "\n",
        "  # Load the model and tokenizer\n",
        "  tokenizer, model = load_model()\n",
        "\n",
        "  # Generate the answer by passing in the retrived docs and llm\n",
        "  generate_clean_with_rag(user_msg, retriever, tokenizer, model)"
      ],
      "metadata": {
        "id": "oaNWmJIYw4kr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_pipeline()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B25Wi9hT3TEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Questions and answers by our model:\n",
        "\n",
        "Q: Is creatine monohydrate dangerous?\n",
        "\n",
        "A: Creatine is a substance that occurs naturally in the body and is found in small amounts in foods such as meat and fish. It is also available as a dietary supplement. Some people take creatine to increase muscle mass and strength. There is no evidence that creatine is dangerous. However, it can cause some side effects, such as weight gain, water retention, and stomach cramps. It can also interact with some medications, so it is important to talk to a doctor before taking creatine.\n",
        "\n",
        "\n",
        "Q: How is asthma diagnosed in children?\n",
        "\n",
        "A: In the United States, the diagnosis of asthma is based on the patient's history and a physical examination. The American Academy of Allergy, Asthma and Immunology (AAAAI) has a \"Blue Ribbon Panel\" of experts in the field of asthma who reviewed the evidence and made recommendations regarding the diagnosis of asthma. The panel recommended that children should be diagnosed with asthma if they have had recurrent episodes of wheezing, chest tightness, coughing, and shortness of breath. The symptoms should have been present for at least 3 months and should be intermittent (come and go). The symptoms should not be due to a known cause, such as a respiratory infection.\n",
        "\n",
        "TESTING WITH FAKE DISEASE:  --> RESULT: Although it does not say it doesn't know abt the disease it says some basic general health advice instead.\n",
        "\n",
        "Q: How do you treat blue vein anemia?\n",
        "\n",
        "A: Hi, I understand your concern. I can suggest you some of the following options. 1. Try adding some fruits and vegetables rich in iron and folic acid. 2. If you are taking any medication, you can consult your doctor and ask him/her to change the medication or prescribe you an iron supplement. 3. Avoid heavy meals, excessive use of tea and coffee. 4. Avoid smoking and alcohol. 5. Try to avoid stress. 6. Avoid contact with people who are suffering from infectious diseases. 7. Avoid prolonged exposure to sunlight. 8. Avoid dehydration. 9. Get a physical examination done. 10. Consult a doctor and get your blood tested. Hope I have answered your query. Let me know if I can assist you further. Take care Chat Doctor"
      ],
      "metadata": {
        "id": "qSLDi5Zl_AGW"
      }
    }
  ]
}