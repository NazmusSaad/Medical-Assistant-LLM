# -*- coding: utf-8 -*-
"""MedLlama: Fine-Tuning LLaMA 2 for Instruction-Based Clinical QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TkAq06Fpq9JepDQz0-HyTGLBBrjFzX3i
"""

!pip install datasets
!pip install transformers -U
!pip install accelerate -U
!pip install trl
!pip install bitsandbytes # for quantization

!pip install peft # for LoRA

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# import bitsandbytes config
import bitsandbytes as bnb
from transformers import BitsAndBytesConfig

# quantization configuration for loading the model in 4-bit
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # model will be stored using normalized 4 bit floats
    bnb_4bit_compute_dtype="float16"  # model will be computed using higher precision data type
)

# Quantization-aware model loading
import transformers
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer

MODEL_NAME = "NousResearch/Llama-2-7b-hf"

# load the LLaMA 2 model with 4-bit quantization using the config defined earlier
model = AutoModelForCausalLM.from_pretrained(
  MODEL_NAME,
  quantization_config=quantization_config,
  device_map="auto"
)

# load corresponding tokenizer
tokenizer = AutoTokenizer.from_pretrained(
  MODEL_NAME,
  trust_remote_code=True
)

# set pading for compatability
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Disable cache while training (conflicts with gradient checkpointing)
model.config.use_cache = False

# Import LoRA-related utilities from PEFT (Parameter-Efficient Fine-Tuning) library
from peft import (
  LoraConfig,
  get_peft_model,
  prepare_model_for_kbit_training
)

# define LoRA config
peft_config = LoraConfig(
  r=16,
  lora_alpha=32,
  target_modules=['q_proj', 'k_proj', 'down_proj' , 'v_proj', 'gate_proj' , 'o_proj' , 'up_proj'],
  lora_dropout=0.05,
  task_type="CAUSAL_LM"
)

# Prepare the quantized model for LoRA training
model = prepare_model_for_kbit_training(model)

# Inject the LoRA adapters into the model
model = get_peft_model(model, peft_config) # all layers except attention layers are frozen

generation_configuration = model.generation_config
generation_configuration.pad_token_id = tokenizer.eos_token_id
generation_configuration.eos_token_id = tokenizer.eos_token_id
generation_configuration.max_new_tokens = 256
generation_configuration.temperature = 0.7
generation_configuration.top_p = 0.9
generation_configuration.do_sample = True
model.config.use_cache = True

# data preprocessing

# load dataset
from datasets import load_dataset
DATASET_NAME = "nlpie/Llama2-MedTuned-Instructions"

SYSTEM_PROMPT = (
    "You are an expert medical assistant. "
    "Answer factually and precisely.\n"
)

def construct_datapoint(example):
    """
    Builds a single training string in Llama-2 chat format, then
    tokenises it so the dataset already contains `input_ids` and
    `attention_mask` (columns the model expects).
    """
    input_text = example.get('input', '').strip()
    input_part = f"\n\n{input_text}" if input_text else ""  # Missing Input Handling

    chat = (
        f"<s>[INST] <<SYS>>\n{SYSTEM_PROMPT}<</SYS>>\n\n"
        f"{example['instruction'].strip()}{input_part} [/INST] "
        f"{example['output'].strip()} </s>"
    )
    return tokenizer(
        chat,
        max_length=1024,
        truncation=True,
        padding="max_length"
    )

# load and shuffle full training split
ds = load_dataset("nlpie/Llama2-MedTuned-Instructions", split="train")

train_test = ds.train_test_split(test_size=0.1, seed=42)

train_ds = (
    train_test['train'].shuffle(seed=42)
    .select(range(2_000))  # Now you get 2,000 training samples
    .map(construct_datapoint, batched=False, remove_columns=train_test['train'].column_names)
)

eval_ds = (
    train_test['test'].shuffle(seed=42)
    .select(range(200))  # 200 eval samples
    .map(construct_datapoint, batched=False, remove_columns=train_test['test'].column_names)
)

# set up training arguments
train_arguments = transformers.TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4, # simulate a larger batch size
    num_train_epochs=1,
    learning_rate=2e-4,
    fp16=True,
    optim="paged_adamw_8bit",
    lr_scheduler_type="cosine",
    warmup_ratio=0.05,
    output_dir="med_finetune",
    save_steps=500,              # Save checkpoints
    eval_steps=500,              # Evaluate periodically
    eval_strategy="steps", # ADD THIS LINE - Required for load_best_model_at_end
    logging_steps=100,           # Log more frequently
    save_total_limit=3,          # Keep only 3 checkpoints
    load_best_model_at_end=True, # Load best model after training
)


trainer = transformers.Trainer(
  model=model,
  train_dataset=train_ds,
  eval_dataset=eval_ds,
  data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
  args=train_arguments
)

model.config.use_cache = False

# train
if __name__ == "__main__":
    try:
      trainer.train()
      trainer.save_model("med_finetune/final")
      print("Training completed successfully!")
    except Exception as e:
      print(f"Training failed: {e}")
      # Save intermediate checkpoint
      trainer.save_model("med_finetune/interrupted")

    # enable cache for fast decoding
    model.gradient_checkpointing_disable()
    model.config.use_cache = True
    model.eval()

def generate_improved(prompt: str, max_new: int = 200):
    """
    Improved generation function with better formatting and stopping criteria
    """
    chat_prompt = (
        f"<s>[INST] <<SYS>>\n{SYSTEM_PROMPT}<</SYS>>\n\n"
        f"{prompt} [/INST]"
    )

    inputs = tokenizer(chat_prompt, return_tensors="pt").to(model.device)

    with torch.inference_mode():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new,
            temperature=0.3,           # Lower temperature for more focused responses
            top_p=0.8,                 # Slightly lower top_p
            do_sample=True,
            repetition_penalty=1.1,    # Lower repetition penalty
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3,    # Prevent repeating 3-grams
            early_stopping=True        # Stop early when EOS is generated
        )

    # Decode and clean up the response
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the assistant's response (after [/INST])
    if "[/INST]" in full_response:
        response = full_response.split("[/INST]")[1].strip()
    else:
        response = full_response

    # Clean up any remaining artifacts
    response = response.replace("</s>", "").strip()

    print("=" * 50)
    print("QUESTION:", prompt)
    print("=" * 50)
    print("ANSWER:")
    print(response)
    print("=" * 50)

    return response

# Test the improved function
generate_improved("Explain the difference between Type 1 and Type 2 diabetes.")

# save model

import os
from huggingface_hub import HfApi, HfFolder
import shutil

def save_model_for_export(model, tokenizer, save_path="./medical_llama_model"):
    """
    Save model and tokenizer for easy export
    """
    # Create directory if it doesn't exist
    os.makedirs(save_path, exist_ok=True)

    # Save the fine-tuned model (LoRA adapters)
    model.save_pretrained(save_path)

    # Save tokenizer
    tokenizer.save_pretrained(save_path)

    # Save a configuration file for easy loading
    config = {
        "base_model": "NousResearch/Llama-2-7b-hf",
        "model_type": "peft",
        "task": "medical_qa",
        "system_prompt": SYSTEM_PROMPT
    }

    import json
    with open(f"{save_path}/config.json", "w") as f:
        json.dump(config, f, indent=2)

    print(f"Model saved to: {save_path}")
    print("Files saved:")
    for file in os.listdir(save_path):
        print(f"  - {file}")

    return save_path

# Save your trained model
save_path = save_model_for_export(model, tokenizer)

"""Save model in HF repo"""

from huggingface_hub import notebook_login

notebook_login()  # This will prompt you to paste your HF token interactively

from huggingface_hub import HfApi

api = HfApi(token=os.getenv("HF_TOKEN"))
api.upload_folder(
    folder_path="medical_llama_model",
    repo_id="Nazmoose/MedLlama-LoRA",
    repo_type="model",
    commit_message="Upload MedTuned LoRA adapter"
)

"""# This is for testing the model after loading it from Hugging face."""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

HF_REPO = "Nazmoose/MedLlama-LoRA"
BASE_REPO = "NousResearch/Llama-2-7b-hf"

def load_model():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # Load tokenizer from your LoRA adapter repo (inherits from base)
    tokenizer = AutoTokenizer.from_pretrained(HF_REPO, token=True)  # set `token=True` for Colab auth

    # Load base LLaMA model in 4-bit
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_REPO,
        quantization_config=bnb_config,
        device_map="auto",
        token=True
    )

    # Load LoRA adapters into base model
    model = PeftModel.from_pretrained(base_model, HF_REPO, token=True)
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

SYSTEM_PROMPT = (
    "You are MedLLaMA, a model fine-tuned for clinical Q&A. "
    "Respond with medically relevant answers but do not provide professional advice."
)

user_msg = input("Ask a medical question:\n> ")

prompt = f"<s>[INST] <<SYS>>\n{SYSTEM_PROMPT}\n<</SYS>>\n\n{user_msg} [/INST]"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.inference_mode():
    output = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

# Decode only the generated response (excluding prompt)
response = tokenizer.decode(
    output[0][inputs["input_ids"].shape[-1]:],
    skip_special_tokens=True
)

print("\nAnswer:\n", response.strip())

def clean_response(raw_text: str) -> str:
    """
    Extracts only the first ###Answer: section from the model output.
    """
    if "###Answer:" in raw_text:
        answer_section = raw_text.split("###Answer:")[1].strip()
        # Optionally stop at next metadata marker
        for stop_token in ["###Rationale", "###Source", "###Tags", "###Used in"]:
            if stop_token in answer_section:
                answer_section = answer_section.split(stop_token)[0].strip()
                break
        return answer_section
    else:
        return raw_text.strip()

def generate_clean():
  SYSTEM_PROMPT = (
      "You are MedLLaMA, a model fine-tuned for clinical Q&A. "
      "Respond with medically relevant answers but do not provide professional advice."
      "Answer the following medical question. End your answer after a few sentences. Do not repeat."
  )

  user_msg = input("Ask a medical question:\n> ")

  prompt = f"<s>[INST] <<SYS>>\n{SYSTEM_PROMPT}\n<</SYS>>\n\n{user_msg} [/INST]"

  inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

  with torch.inference_mode():
      output = model.generate(
          **inputs,
          max_new_tokens=512,
          temperature=0.7,
          top_p=0.9,
          pad_token_id=tokenizer.eos_token_id,
          eos_token_id=tokenizer.eos_token_id,
      )

  # Decode only the generated response (excluding prompt)
  response = tokenizer.decode(
      output[0][inputs["input_ids"].shape[-1]:],
      skip_special_tokens=True
  )
  cleaned = clean_response(response.strip())

  print(cleaned)

generate_clean()